{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 吴恩达老师机器学习课后练习————神经网络(黄海广博士版本)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 此练习是基于5000个feature & label的手写体识别神经网络模型；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 我们通过反向传播算法实现神经网络代价函数和梯度计算的正则化和非正则化版本，并实现随机权重初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sio.loadmat('ex4data1.mat')\n",
    "\n",
    "X = data['X']        # (5000, 400)\n",
    "y = data['y']        # (5000, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 将包含10种不同类别的5000个标签向量化成二进制形式，sklearn库中有现成的模块，详见配图 OneHot.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoder.fit(y)\n",
    "y_onehot = encoder.transform(y)        # (5000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 构建神经网络，输入层(400特征+1偏置单元)，1个隐藏层(25个单元+1个偏置)，输出层(10个特征单元)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 定义sigmoid 核函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前向传播：400+1 ---> 25+1 ---> 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagate(X, theta1, theta2):\n",
    "    m = X.shape[0]\n",
    "    a1 = np.insert(X, 0, values=np.ones(m), axis=1)\n",
    "    z2 = a1 @ theta1.T\n",
    "    a2 = np.insert(sigmoid(z2), 0, values=np.ones(m), axis=1)\n",
    "    z3 = a2 @ theta2.T\n",
    "    h = sigmoid(z3)        # (5000, 10)\n",
    "    return a1, z2, a2, z3, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 初始化设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 400\n",
    "hidden_size = 25\n",
    "num_labels = 10\n",
    "learning_rate = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1~2层共25×401个theta，2~3层共10×26个theta，所以共生成 25×401+10×26 个 在(-eps, +eps)之间的数作为随机初始化theta。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### theta1截取前25×401个，theta2截取剩下的10×26个。简化：random()×2×eps-eps = 2×eps×(random()-0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = (np.random.random(size=hidden_size * (input_size + 1) + num_labels * (hidden_size + 1)) - 0.5) * 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 正则化代价函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(params, X, y, input_size, hidden_size, num_labels, learning_rate):\n",
    "    m = X.shape[0]\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "\n",
    "    theta1 = np.reshape(params[ : hidden_size*(input_size+1)], (hidden_size, (input_size+1)))\n",
    "    theta2 = np.reshape(params[hidden_size*(input_size+1) : ], (num_labels, (hidden_size+1)))\n",
    "\n",
    "    a1, z2, a2, z3, h = forward_propagate(X, theta1, theta2)\n",
    "\n",
    "    J = 0\n",
    "    for i in range(m):\n",
    "        first_term = np.multiply(-y[i, :], np.log(h[i, :]))\n",
    "        second_term = np.multiply((1-y[i, :]), np.log(1-h[i, :]))\n",
    "        J += np.sum(first_term - second_term)\n",
    "\n",
    "    J /= m\n",
    "\n",
    "    J += (float(learning_rate)/(2*m)) * (np.sum(np.power(theta1[1:], 2)) + np.sum(np.power(theta2[1:], 2)))\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 反向传播算法："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 先定义 sigmoid 函数的导数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_gradient(z):\n",
    "    return np.multiply(sigmoid(z), (1-sigmoid(z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 实施反向传播来计算梯度。反向传播所需的计算就是代价函数中所需的计算过程，在此拓展函数执行反向传播返回梯度和代价"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 误差：  $\\delta^{(3)}=a^{(3)}-y$，$\\delta^{(2)}=(\\theta^{(2)})^T\\delta^{(3)}*g'(z^{(3)})$ ，$\\delta^{(1)}$没有误差(输入层)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 则用误差表示偏导数（未正则化）： $\\frac{\\partial}{\\partial\\Theta_{ij}^{(l)}}J(\\Theta)=a_{j}^{(l)} \\delta_{i}^{l+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 最后计算偏导数矩阵（含正则化）： $ \\frac{\\partial}{\\partial\\Theta^{(l)}_{ij}}=D_{ij}^{(l)}=\\frac{1}{m}\\Delta_{ij}^{(l)}+\\frac{\\lambda}{m}\\Theta_{ij}^{(l)}$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ${if}\\; j \\neq  0$  \n",
    "##### &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  $\\frac{\\partial}{\\partial\\Theta^{(l)}_{ij}}=D_{ij}^{(l)}=\\frac{1}{m}\\Delta_{ij}^{(l)}$  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  ${if}\\; j = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(params, X, y, input_size, hidden_size, num_labels, learning_rate):\n",
    "    m = X.shape[0]\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "\n",
    "    theta1 = np.matrix(np.reshape(params[ : hidden_size*(input_size+1)], (hidden_size, (input_size+1))))\n",
    "    theta2 = np.matrix(np.reshape(params[hidden_size*(input_size+1) : ], (num_labels, (hidden_size+1))))\n",
    "    a1, z2, a2, z3, h = forward_propagate(X, theta1, theta2)\n",
    "\n",
    "    J = 0\n",
    "    # 初始化误差\n",
    "    delta1 = np.zeros(theta1.shape)    # (25, 401)\n",
    "    delta2 = np.zeros(theta2.shape)    # (10, 26)\n",
    "\n",
    "    # compute the cost\n",
    "    for i in range(m):\n",
    "        first_term = np.multiply(-y[i, :], np.log(h[i, :]))\n",
    "        second_term = np.multiply((1-y[i, :]), np.log(1-h[i, :]))\n",
    "        J += np.sum(first_term - second_term)\n",
    "\n",
    "    J /= m\n",
    "\n",
    "    J += (float(learning_rate)/(2*m)) * (np.sum(np.power(theta1[:, 1:], 2)) + np.sum(np.power(theta2[:, 1:], 2)))\n",
    "\n",
    "    # perform backpropagation\n",
    "    for t in range(m):\n",
    "        a1t = a1[t, :]    # (1, 401)\n",
    "        z2t = z2[t, :]    # (1, 25)\n",
    "        a2t = a2[t, :]    # (1, 26)\n",
    "        ht = h[t, :]      # (1, 10)\n",
    "        yt = y[t, :]      # (1, 10)\n",
    "\n",
    "        d3t = ht - yt        # 最后一层的误差 delta    (1, 10)\n",
    "\n",
    "        z2t = np.insert(z2t, 0, values=np.ones(1))    # (1, 26)\n",
    "        d2t = np.multiply((theta2.T @ d3t.T).T, sigmoid_gradient(z2t))    # 第二层误差 (1, 26)\n",
    "\n",
    "        delta1 = delta1 + d2t[:,1:].T @ a1t\n",
    "        delta2 = delta2 + d3t.T @ a2t\n",
    "        \n",
    "    delta1 = delta1 / m\n",
    "    delta2 = delta2 / m\n",
    "    \n",
    "    # add the grandient regularization term\n",
    "    delta1[:, 1:] = delta1[:, 1:] + (theta1[:, 1:]*learning_rate) / m\n",
    "    delta2[:, 1:] = delta2[:, 1:] + (theta2[:, 1:]*learning_rate) / m\n",
    "    \n",
    "    grad = np.concatenate((np.ravel(delta1), np.ravel(delta2)))\n",
    "    \n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 终于完成了神经网络的过程，接下来用数据集训练该神经网络，并用它进行预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 由于将代价和梯度都在反向传播函数中返回了，所以 jac=True。若fun中只返回代价，则jac=梯度。详见官方文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: 0.3343120231391213\n",
      "     jac: array([-1.32377894e-04,  7.83180650e-07,  6.39625209e-07, ...,\n",
      "       -3.82279414e-05,  1.81743812e-05, -6.39246575e-05])\n",
      " message: 'Max. number of function evaluations reached'\n",
      "    nfev: 250\n",
      "     nit: 23\n",
      "  status: 3\n",
      " success: False\n",
      "       x: array([-0.71613203,  0.0039159 ,  0.00319813, ..., -1.9969562 ,\n",
      "       -2.3056285 ,  0.97026344])\n",
      "Wall time: 3min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from scipy.optimize import minimize\n",
    "# minimize the objective function\n",
    "fmin = minimize(fun=backprop, x0=params, args=(X, y_onehot, input_size, hidden_size, num_labels, learning_rate), method='TNC', jac=True, options={\"maxiter\":250})\n",
    "print(fmin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 由于目标函数不太可能完全收敛，我们对迭代次数进行了限制。总代价已经下降到0.5以下，这是算法正常工作的一个很好的指标。 我们使用这些参数，通过网络传播，获得一些预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 使用找到的参数，通过前向传播获得预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.matrix(X)\n",
    "theta1 = np.matrix(np.reshape(fmin.x[:hidden_size*(input_size+1)], (hidden_size, (input_size+1))))\n",
    "theta2 = np.matrix(np.reshape(fmin.x[hidden_size*(input_size+1):], (num_labels, (hidden_size+1))))\n",
    "\n",
    "a1, z2, a2, z3, h = forward_propagate(X, theta1, theta2)\n",
    "\n",
    "y_pred = np.array(np.argmax(h, axis=1) + 1)        # 很巧妙，详见配图 onehot下标.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.99      0.99      0.99       500\n",
      "          2       0.99      1.00      0.99       500\n",
      "          3       0.99      0.99      0.99       500\n",
      "          4       1.00      0.99      0.99       500\n",
      "          5       1.00      0.99      0.99       500\n",
      "          6       1.00      0.99      1.00       500\n",
      "          7       0.99      0.99      0.99       500\n",
      "          8       1.00      1.00      1.00       500\n",
      "          9       0.99      0.99      0.99       500\n",
      "         10       0.99      1.00      1.00       500\n",
      "\n",
      "avg / total       0.99      0.99      0.99      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 下一章开始学习支持向量机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
